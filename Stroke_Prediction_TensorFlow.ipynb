{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tensorflow to build a CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to re-import my data to turn it into a tensor\n",
    "tensor_data = pd.read_csv('/Users/dinuka/Downloads/healthcare-dataset-stroke-data 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Cleaning data before creating tensors\n",
    "#dropping id column as we do not need unique identifier\n",
    "tensor_data.drop('id', inplace = True , axis = 1)\n",
    "\n",
    "# Filling in the null values of BMI with the average values\n",
    "tensor_data['bmi'].fillna(tensor_data['bmi'].mean(), inplace = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features and labels\n",
    "stroke_features = tensor_data.copy()\n",
    "stroke_labels = stroke_features.pop('stroke')\n",
    "\n",
    "#Splitting data sets in training and test \n",
    "stroke_features, stroke_features_test, stroke_labels, stroke_labels_test = train_test_split(stroke_features, stroke_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a symbolic input object\n",
    "# Creating input function \n",
    "inputs = {}\n",
    "for name, column in stroke_features.items():\n",
    "    dtype = column.dtype\n",
    "    if dtype == object:\n",
    "        dtype = tf.string\n",
    "    else: \n",
    "        dtype = tf.float32\n",
    "    \n",
    "    inputs[name] = tf.keras.Input(shape=(1,), name = name, dtype = dtype)\n",
    "    \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a symbolic input object for test data\n",
    "# Creating input function \n",
    "inputs = {}\n",
    "for name, column in stroke_features_test.items():\n",
    "    dtype = column.dtype\n",
    "    if dtype == object:\n",
    "        dtype = tf.string\n",
    "    else: \n",
    "        dtype = tf.float32\n",
    "    \n",
    "    inputs[name] = tf.keras.Input(shape=(1,), name = name, dtype = dtype)\n",
    "    \n",
    "inputs\n",
    "\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating Numeric Inputs and Normalizing\n",
    "numeric_inputs = {name:input for name, input in inputs.items()\n",
    "                    if input.dtype == tf.float32}\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(tensor_data[numeric_inputs.keys()]))\n",
    "train_numeric_inputs = norm(x)\n",
    "\n",
    "train_numeric_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating Numeric Inputs and Normalizing for test data\n",
    "numeric_inputs = {name:input for name, input in inputs.items()\n",
    "                    if input.dtype == tf.float32}\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(tensor_data[numeric_inputs.keys()]))\n",
    "test_numeric_inputs = norm(x)\n",
    "\n",
    "test_numeric_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all the pre-processed results train\n",
    "preprocessed_inputs_train = [train_numeric_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all the pre-processed results test\n",
    "preprocessed_inputs_test = [test_numeric_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using string-lookup and category encoding to convert string data to indexed float32\n",
    "for name, input in inputs.items():\n",
    "    if input.dtype == tf.float32:\n",
    "        continue\n",
    "        \n",
    "    lookup = layers.StringLookup(vocabulary=np.unique(stroke_features[name]))\n",
    "    one_hot = layers.CategoryEncoding(max_tokens=lookup.vocab_size())\n",
    "    \n",
    "    x = lookup(input)\n",
    "    x = one_hot(x)\n",
    "    \n",
    "    preprocessed_inputs_train.append(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using string-lookup and category encoding to convert string data to indexed float32 for test data\n",
    "for name, input in inputs.items():\n",
    "    if input.dtype == tf.float32:\n",
    "        continue\n",
    "        \n",
    "    lookup = layers.StringLookup(vocabulary=np.unique(stroke_features[name]))\n",
    "    one_hot = layers.CategoryEncoding(max_tokens=lookup.vocab_size())\n",
    "    \n",
    "    x = lookup(input)\n",
    "    x = one_hot(x)\n",
    "    \n",
    "    preprocessed_inputs_test.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install pydot and graphviz\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all preprocessed inputs train\n",
    "preprocessed_inputs_train_cat = layers.Concatenate()(preprocessed_inputs_train)\n",
    "\n",
    "\n",
    "# Concatenating all preprocessed inputs test\n",
    "preprocessed_inputs_test_cat = layers.Concatenate()(preprocessed_inputs_test)\n",
    "\n",
    "\n",
    "\n",
    "stroke_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_train_cat)\n",
    "\n",
    "#tf.keras.utils.plot_model(model = stroke_preprocessing, rankdir=\"LR\", dpi = 72, show_shapes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of tensors\n",
    "stroke_features_dict = {name: np.array(value)\n",
    "                       for name, value in stroke_features.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of tensors for test data\n",
    "stroke_test_features_dict = {name: np.array(value)\n",
    "                       for name, value in stroke_features_test.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a slice of the first training example to pass it through model\n",
    "features_dict = {name:values[:1] for name, values in stroke_features_dict.items()}\n",
    "stroke_preprocessing(features_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now building model\n",
    "def stroke_model(preprocessing_head, inputs):\n",
    "    body = tf.keras.Sequential([\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    preprocessed_inputs = preprocessing_head(inputs)\n",
    "    result = body(preprocessed_inputs)\n",
    "    model = tf.keras.Model(inputs, result)\n",
    "    \n",
    "    model.compile(loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                                                   optimizer = tf.optimizers.Adam()\n",
    "                                                  )\n",
    "    \n",
    "    return model\n",
    "\n",
    "stroke_model = stroke_model(stroke_preprocessing, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with dictionary of features x and labels y\n",
    "stroke_model.fit(x = stroke_features_dict, y = stroke_labels, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating loss function of test results\n",
    "\n",
    "test_results = stroke_model.evaluate(x = stroke_test_features_dict, y = stroke_labels_test)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting based on test inputs and finding precision and f1 scores\n",
    "keras_predict = stroke_model.predict(stroke_test_features_dict)\n",
    "\n",
    "print('Accuracy --> ',accuracy_score(random_forest_predict,test_y))\n",
    "print('F1 Score --> ',f1_score(random_forest_predict,test_y))\n",
    "print('Classification Report  --> \\n',classification_report(random_forest_predict,test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
